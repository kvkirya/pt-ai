{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc5c7998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from opencv-python-headless) (1.26.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: mediapipe in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (0.10.8)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from mediapipe) (4.8.1.78)\n",
      "Requirement already satisfied: numpy in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from mediapipe) (1.26.2)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: matplotlib in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from mediapipe) (3.8.2)\n",
      "Requirement already satisfied: absl-py in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from mediapipe) (2.0.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from matplotlib->mediapipe) (4.45.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from matplotlib->mediapipe) (3.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from matplotlib->mediapipe) (10.1.0)\n",
      "Requirement already satisfied: pycparser in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nicowsendagorta/.pyenv/versions/3.10.6/envs/pt-ai/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python-headless\n",
    "!pip install mediapipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ae2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8038d9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1701260999.970424       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 83), renderer: Apple M2 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1701260999.981595       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 83), renderer: Apple M2 Pro\n"
     ]
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face = mp.solutions.face_detection\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Face Detection and Pose Estimation models\n",
    "face_detection = mp_face.FaceDetection(min_detection_confidence=0.5)\n",
    "pose_model = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448cfd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=FsVAvgR9ifY\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # Convert to rgb because if not movenet does not capture it. ask lorcan about this bad boy\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    face_results = face_detection.process(frame_rgb)\n",
    "    results = pose_model.process(frame_rgb)\n",
    "\n",
    "# results.(pose_landmarks, pose_world_landmarks, segmentation_mask)\n",
    "    if results.pose_landmarks:   # could be multi_hand_landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=10, circle_radius=2),\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=10, circle_radius=2),\n",
    ")\n",
    "\n",
    "# mp_pose is mp.solutions.pose. POSE_CONNECTIONS is part of doc only option\n",
    "\n",
    "    if face_results.detections:\n",
    "        for detection in face_results.detections:\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "            ih, iw, _ = frame.shape\n",
    "            x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # mp_drawing.draw_landmarks(image: numpy.ndarray,\n",
    "#     landmark_list: mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList,\n",
    "#     connections: Optional[List[Tuple[int, int]]] = None,\n",
    "#     landmark_drawing_spec: Union[mediapipe.python.solutions.drawing_utils.DrawingSpec, Mapping[int, mediapipe.python.solutions.drawing_utils.DrawingSpec]] = DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2),\n",
    "#     connection_drawing_spec: Union[mediapipe.python.solutions.drawing_utils.DrawingSpec, Mapping[Tuple[int, int], mediapipe.python.solutions.drawing_utils.DrawingSpec]] = DrawingSpec(color=(224, 224, 224), thickness=2, circle_radius=2),\n",
    "# )\n",
    "\n",
    "\n",
    "    cv2.imshow('Body Detector', frame)\n",
    "\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "#kill cam from cv2 when break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc9150f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b854826",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this code has both but bad pose estimation. therefore implement this code from stack onto mine above\n",
    "##this code also does the body based on face refcognition which is something we dont want, as the face might not be visible from side perspective\n",
    "## https://stackoverflow.com/questions/77426154/how-to-find-the-face-orientation-using-mediapipe\n",
    "\n",
    "# while True:\n",
    "#     success, frame = cap.read()\n",
    "#     if not success:\n",
    "#         continue\n",
    "\n",
    "#     # Convert the frame to RGB format for MediaPipe\n",
    "#     frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Perform face detection\n",
    "#     face_results = face_detection.process(frame_rgb)\n",
    "\n",
    "#     if face_results.detections:\n",
    "#         for detection in face_results.detections:\n",
    "#             bboxC = detection.location_data.relative_bounding_box\n",
    "#             ih, iw, _ = frame.shape\n",
    "#             x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "#             cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "#             # Perform pose estimation using the face region\n",
    "#             face_center = (x + w // 2, y + h // 2)\n",
    "#             frame_pose = frame[y:y + h, x:x + w]\n",
    "#             frame_pose_rgb = cv2.cvtColor(frame_pose, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#             # Perform pose estimation on the face region\n",
    "#             pose_results = pose_estimation.process(frame_pose_rgb)\n",
    "\n",
    "#             if pose_results.pose_landmarks:\n",
    "#                 # You can access pose landmarks and orientation information here\n",
    "#                 landmarks = pose_results.pose_landmarks\n",
    "#                 # Extract relevant pose landmarks and calculate face orientation\n",
    "\n",
    "#                 # Draw landmarks and orientation lines\n",
    "#                 mp_drawing.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "#     cv2.imshow(\"Face Orientation Detection\", frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad1b9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37822d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ccc55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12882a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
