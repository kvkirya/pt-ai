{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205cefef",
   "metadata": {},
   "source": [
    "# Requirements üìã‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed4368d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 15:39:24.214380: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-06 15:39:24.859652: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-12-06 15:39:24.859670: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-12-06 15:39:24.936769: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-06 15:39:26.850104: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-06 15:39:26.850818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-12-06 15:39:26.850834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "# import cv2\n",
    "\n",
    "# Needed for the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow import convert_to_tensor\n",
    "from tensorflow.image import rgb_to_grayscale\n",
    "from tensorflow.keras.models import load_model as tfk__load_model\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d476464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc6f4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import convert_to_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee526c",
   "metadata": {},
   "source": [
    "# Hopefully fixing cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41e7fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5831431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4be41ad",
   "metadata": {},
   "source": [
    "# The Dataset location üìç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Data Load ‚è≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "validation_split = 0.2\n",
    "num_classes = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4772 files belonging to 4 classes.\n",
      "Using 3818 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 14:09:02.421058: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "train_dataset = image_dataset_from_directory(\n",
    "    directory='/home/kyrill/code/pt-ai/pt-ai/raw_data/processed_data_03',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    validation_split=validation_split,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4772 files belonging to 4 classes.\n",
      "Using 954 files for validation.\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = image_dataset_from_directory(\n",
    "    directory='/home/kyrill/code/pt-ai/pt-ai/raw_data/processed_data_03',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    validation_split=validation_split,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da8cf4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.prefetch_op._PrefetchDataset"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4099ff1",
   "metadata": {},
   "source": [
    "## Preprocessing ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model Function ü¶æüíªüß†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca18dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_CNN():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Preprocessing layers\n",
    "    model.add(layers.CenterCrop(height=350, width=450, input_shape=[256,256,1]))\n",
    "\n",
    "    # Build of the Model\n",
    "    model.add(layers.Conv2D(filters=8, kernel_size=(4,4), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D((2,2)))\n",
    "    model.add(layers.Conv2D(16,(3,3), activation='relu'))\n",
    "    model.add(layers.MaxPool2D((2,2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "    # Compilation of the Model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc4999b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " center_crop (CenterCrop)    (None, 350, 450, 1)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 350, 450, 8)       136       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 175, 225, 8)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 173, 223, 16)      1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 86, 111, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 152736)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1527370   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1528718 (5.83 MB)\n",
      "Trainable params: 1528718 (5.83 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary of the CNN model\n",
    "tmp_first_CNN = initialize_CNN()\n",
    "tmp_first_CNN.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7c196b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=1, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6b122aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 195s 2s/step - loss: 1.3670 - accuracy: 0.3279 - val_loss: 1.3507 - val_accuracy: 0.3532\n"
     ]
    }
   ],
   "source": [
    "history = tmp_first_CNN.fit(train_dataset,\n",
    "                            epochs=1,\n",
    "                            validation_data=validation_dataset,\n",
    "                            batch_size=16,\n",
    "                            callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 9s 285ms/step - loss: 1.3507 - accuracy: 0.3532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.350656270980835, 0.3532494902610779]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_first_CNN.evaluate(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_images(image_path):\n",
    "    pred_image = Image.open(image_path)\n",
    "    X_pred = np.array(pred_image)\n",
    "    X_pred = tf.convert_to_tensor(X_pred, dtype=tf.float32)\n",
    "    X_pred = tf.image.rot90(X_pred, k=-1)\n",
    "\n",
    "    X_pred = X_pred[:,:,:3]/255.\n",
    "    X_pred = tf.image.resize_with_pad(X_pred, 256,256)\n",
    "    X_pred = rgb_to_grayscale(X_pred)\n",
    "\n",
    "    X_pred = tf.expand_dims(X_pred, axis=0)\n",
    "\n",
    "    # plt.imshow(rgb_to_grayscale(X_pred[:,:,:3]/255.), cmap='gray')\n",
    "    return X_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 256, 256, 1), dtype=float32, numpy=\n",
       "array([[[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_images('../raw_data/test_images/IMG_8806.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_model(model, image_preprocessed):\n",
    "    \"\"\"Try and predict an image from the Dataset\"\"\"\n",
    "\n",
    "    y_pred = model.predict(image_preprocessed)\n",
    "    print(f'‚úÖ Prediction complete. Pose: {y_pred}')\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 254ms/step\n",
      "‚úÖ Prediction complete. Pose: [[0.23731439 0.09497999 0.5056726  0.1620331 ]]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "‚úÖ Prediction complete. Pose: [[0.23759201 0.09759529 0.49646252 0.16835018]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "‚úÖ Prediction complete. Pose: [[0.23676254 0.09712522 0.49954796 0.16656429]]\n",
      "[[0.23731439 0.09497999 0.5056726  0.1620331 ]] [[0.23759201 0.09759529 0.49646252 0.16835018]] [[0.23676254 0.09712522 0.49954796 0.16656429]]\n"
     ]
    }
   ],
   "source": [
    "fitted_model = tfk__load_model('../raw_data/models/model.h5')\n",
    "res1 = prediction_model(fitted_model, preprocessing_images('../raw_data/test_images/IMG_8803 (1).jpg'))\n",
    "res2 = prediction_model(fitted_model, preprocessing_images('../raw_data/test_images/IMG_8804.jpg'))\n",
    "res3 = prediction_model(fitted_model, preprocessing_images('../raw_data/test_images/IMG_8806.jpg'))\n",
    "\n",
    "print(res1, res2, res3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Server Error\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# URL of your FastAPI endpoint\n",
    "url = 'http://0.0.0.0:8000/uploadfile/'\n",
    "\n",
    "# Path to the local image file\n",
    "file_path = \"/home/kyrill/code/pt-ai/pt-ai/raw_data/test_images/IMG_8803 (1).jpg\"\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open(file_path, 'rb') as f:\n",
    "    # Define the file as a dictionary. The key ('file' in this case)\n",
    "    # should match the name of the parameter in your FastAPI endpoint\n",
    "    files = {'file': (file_path, f, 'image/jpeg')}\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(url, files=files)\n",
    "\n",
    "# Print the response from the server\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "id": "4693c081",
=======
   "execution_count": 5,
>>>>>>> mischa-final-1
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
   "id": "4fe99e5d",
=======
   "execution_count": 6,
   "id": "71a51bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://0.0.0.0:8000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
>>>>>>> mischa-final-1
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'keypoints': '{\"left_elbow_a\":{\"0\":98.206},\"right_elbow_a\":{\"0\":63.144},\"left_shoulder_b\":{\"0\":63.283},\"left_shoulder_a\":{\"0\":83.587},\"right_shoulder_b\":{\"0\":81.26},\"right_shoulder_a\":{\"0\":79.943},\"left_hip_a\":{\"0\":61.399},\"left_hip_b\":{\"0\":70.232},\"left_hip_c\":{\"0\":38.439},\"right_hip_a\":{\"0\":108.668},\"right_hip_b\":{\"0\":166.948},\"right_hip_c\":{\"0\":73.776},\"left_knee_a\":{\"0\":48.219},\"right_knee_a\":{\"0\":97.831}}', 'keypoints_scores': '\"array([[[[0.25846836, 0.47987658, 0.46073812],\\\\n         [0.23945445, 0.49583453, 0.6491913 ],\\\\n         [0.2426734 , 0.4789639 , 0.629262  ],\\\\n         [0.23364882, 0.54194945, 0.6532644 ],\\\\n         [0.23658925, 0.5085863 , 0.6407261 ],\\\\n         [0.30808482, 0.59806657, 0.73906344],\\\\n         [0.33981377, 0.4906249 , 0.77313393],\\\\n         [0.43789786, 0.61506844, 0.6275884 ],\\\\n         [0.41925642, 0.40572846, 0.42696267],\\\\n         [0.32991466, 0.47951537, 0.4219612 ],\\\\n         [0.3280912 , 0.4367951 , 0.44513044],\\\\n         [0.5278414 , 0.66264653, 0.83311135],\\\\n         [0.52381736, 0.5799902 , 0.76059514],\\\\n         [0.567676  , 0.58138245, 0.41435543],\\\\n         [0.5320914 , 0.48450398, 0.7057464 ],\\\\n         [0.7613534 , 0.6349071 , 0.61685413],\\\\n         [0.702254  , 0.5295398 , 0.5961265 ]]]], dtype=float32)\"'}\n"
     ]
    }
   ],
   "source": [
    "# URL of your FastAPI endpoint\n",
    "url_skeli = f'{url}/skeletonizer'\n",
    "\n",
    "# Path to the local image file\n",
    "file_path = \"/home/kyrill/code/pt-ai/pt-ai/raw_data/test_images/chris_heria_squat.jpg\"\n",
    "\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open(file_path, 'rb') as f:\n",
    "    # Define the file as a dictionary. The key ('file' in this case)\n",
    "    # should match the name of the parameter in your FastAPI endpoint\n",
    "    #files = {'file': (file_path, f, 'image/jpeg')}\n",
    "    files = {'file': f}\n",
    "    image = Image.open(f)\n",
    "    image = image.resize((1600,1200))\n",
    "    image_array = np.array(image)\n",
    "    # Make the POST request\n",
    "    response = requests.post(f\"{url}/skeletonizer\", json=json.dumps(image_array.tolist()), verify=False)\n",
    "\n",
    "# Print the response from the server\n",
    "print(json.loads(response.text))\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
   "id": "f2b0f9bc",
=======
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'left_elbow_a': {'0': 121.154},\n",
       " 'right_elbow_a': {'0': 66.543},\n",
       " 'left_shoulder_b': {'0': 85.363},\n",
       " 'left_shoulder_a': {'0': 70.072},\n",
       " 'right_shoulder_b': {'0': 107.837},\n",
       " 'right_shoulder_a': {'0': 56.164},\n",
       " 'left_hip_a': {'0': 56.965},\n",
       " 'left_hip_b': {'0': 127.652},\n",
       " 'left_hip_c': {'0': 165.929},\n",
       " 'right_hip_a': {'0': 75.033},\n",
       " 'right_hip_b': {'0': 8.999},\n",
       " 'right_hip_c': {'0': 66.77},\n",
       " 'left_knee_a': {'0': 75.46},\n",
       " 'right_knee_a': {'0': 76.836}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(eval(response.text)['keypoints'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7951d",
   "metadata": {},
   "source": [
    "left_elb_a\n",
    "right_elb_a, left_shoulder_b, right_shoulder_b, left_hip_b, right_hip_c, left_knee_a, right_knee_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
>>>>>>> mischa-final-1
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, float32\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
   "id": "71fb9a8b",
=======
   "execution_count": 12,
   "id": "ce28adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
>>>>>>> mischa-final-1
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.25846836, 0.47987658, 0.46073812],\n",
       "         [0.23945445, 0.49583453, 0.6491913 ],\n",
       "         [0.2426734 , 0.4789639 , 0.629262  ],\n",
       "         [0.23364882, 0.54194945, 0.6532644 ],\n",
       "         [0.23658925, 0.5085863 , 0.6407261 ],\n",
       "         [0.30808482, 0.59806657, 0.73906344],\n",
       "         [0.33981377, 0.4906249 , 0.77313393],\n",
       "         [0.43789786, 0.61506844, 0.6275884 ],\n",
       "         [0.41925642, 0.40572846, 0.42696267],\n",
       "         [0.32991466, 0.47951537, 0.4219612 ],\n",
       "         [0.3280912 , 0.4367951 , 0.44513044],\n",
       "         [0.5278414 , 0.66264653, 0.83311135],\n",
       "         [0.52381736, 0.5799902 , 0.76059514],\n",
       "         [0.567676  , 0.58138245, 0.41435543],\n",
       "         [0.5320914 , 0.48450398, 0.7057464 ],\n",
       "         [0.7613534 , 0.6349071 , 0.61685413],\n",
       "         [0.702254  , 0.5295398 , 0.5961265 ]]]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = eval(eval(eval(response.text)[\"keypoints_scores\"]))\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3d19f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([0.25846836, 0.47987658, 0.46073812], dtype=float32),),\n",
       " (array([0.23945445, 0.49583453, 0.6491913 ], dtype=float32),),\n",
       " (array([0.2426734, 0.4789639, 0.629262 ], dtype=float32),),\n",
       " (array([0.23364882, 0.54194945, 0.6532644 ], dtype=float32),),\n",
       " (array([0.23658925, 0.5085863 , 0.6407261 ], dtype=float32),),\n",
       " (array([0.30808482, 0.59806657, 0.73906344], dtype=float32),),\n",
       " (array([0.33981377, 0.4906249 , 0.77313393], dtype=float32),),\n",
       " (array([0.43789786, 0.61506844, 0.6275884 ], dtype=float32),),\n",
       " (array([0.41925642, 0.40572846, 0.42696267], dtype=float32),),\n",
       " (array([0.32991466, 0.47951537, 0.4219612 ], dtype=float32),),\n",
       " (array([0.3280912 , 0.4367951 , 0.44513044], dtype=float32),),\n",
       " (array([0.5278414 , 0.66264653, 0.83311135], dtype=float32),),\n",
       " (array([0.52381736, 0.5799902 , 0.76059514], dtype=float32),),\n",
       " (array([0.567676  , 0.58138245, 0.41435543], dtype=float32),),\n",
       " (array([0.5320914 , 0.48450398, 0.7057464 ], dtype=float32),),\n",
       " (array([0.7613534 , 0.6349071 , 0.61685413], dtype=float32),),\n",
       " (array([0.702254 , 0.5295398, 0.5961265], dtype=float32),)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped = zip(points[0][0][:])\n",
    "list(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ff6e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predict\":\"[2]\"}\n"
     ]
    }
   ],
   "source": [
    "url = 'http://0.0.0.0:8000/automl_model/'\n",
    "\n",
    "# Path to the local image file\n",
    "dict_var = eval(response.text)\n",
    "input_for_model = {\"data\":dict_var}\n",
    "response = requests.post(url, json=input_for_model)\n",
    "\n",
    "# Print the response from the server\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "b0546e06",
=======
   "execution_count": 2,
>>>>>>> mischa-final-1
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_triangle(x1, x2, x3, y1, y2, y3, z1, z2, z3):\n",
    "\n",
    "    num = (x2-x1)*(x3-x1)+(y2-y1)*(y3-y1)+(z2-z1)*(z3-z1)\n",
    "\n",
    "    den = np.sqrt((x2-x1)**2+(y2-y1)**2+(z2-z1)**2)*\\\n",
    "                np.sqrt((x3-x1)**2+(y3-y1)**2+(z3-z1)**2)\n",
    "\n",
    "    angle = np.degrees(np.arccos(num / den))\n",
    "\n",
    "    return round(angle, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_elbow_l = [0.5467346 , 0.3757556 , 0.4005136 ]\n",
    "point_shoulder_l = [0.5424109 , 0.3640797 , 0.3843048 ]\n",
    "point_hip_l = [0.59016985, 0.48088628, 0.42220116]\n",
    "\n",
    "angle_shoulder = angle_triangle(point_shoulder_l[0], point_hip_l[0], point_elbow_l[0], point_shoulder_l[1], point_hip_l[1], point_elbow_l[1], point_shoulder_l[2], point_hip_l[2], point_elbow_l[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.787"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_shoulder\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
